{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_and_RNN .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayw32FtO0Cgd"
      },
      "source": [
        "### In RNN_CNN.ipynb file, we build CNN and RNN models for image data and noisyTextDescription. After training, we get 3 model files(best.h5, char_rnn_classification_model_lstm.pt and char_rnn_classification_model_gru.pt).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAh58ed4w67N"
      },
      "source": [
        "# Load Data from Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ0B-SGqP_DF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip 'drive/MyDrive/Colab Notebooks/uw-cs480-fall20.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeltJkT_uW9h"
      },
      "source": [
        "# Using CNN to predict categories with images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlP3k6Zugwc"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMzZpac6PXu1"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "def load_data():\n",
        "    train_file = \"uw-cs480-fall20/train.csv\"\n",
        "    test_file = \"uw-cs480-fall20/test.csv\"\n",
        "    df_train = pd.read_csv(train_file)\n",
        "\n",
        "    df_test = pd.read_csv(test_file)\n",
        "    df_test.head()\n",
        "    return df_train, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KlbNUmPPXu1"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "df_train, df_test = load_data()\n",
        "\n",
        "n_train_data = 21627\n",
        "n_test_data = 21628\n",
        "train_id = df_train['id']\n",
        "test_id = df_test['id']\n",
        "train_category = df_train['category']\n",
        "\n",
        "train_img = []\n",
        "test_img = []\n",
        "for i in range(n_train_data):\n",
        "    img = load_img('uw-cs480-fall20/suffled-images/shuffled-images/' + str(train_id[i]) +'.jpg')\n",
        "    train_img.append(img_to_array(img))\n",
        "\n",
        "x_train = np.array(train_img)\n",
        "\n",
        "for i in range(n_test_data):\n",
        "    img = load_img('uw-cs480-fall20/suffled-images/shuffled-images/' + str(test_id[i]) +'.jpg')\n",
        "    test_img.append(img_to_array(img))\n",
        "    \n",
        "x_test = np.array(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wfxSjtFPXu1"
      },
      "source": [
        "train_category = df_train[\"category\"]\n",
        "all_categories = list(set(train_category))\n",
        "n_categories = len(all_categories)\n",
        "y_train = []\n",
        "\n",
        "for i in range(n_train_data):\n",
        "    current_category = train_category[i]\n",
        "    index = all_categories.index(current_category)\n",
        "    y_train.append(index)\n",
        "\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-75ReKuPXu1"
      },
      "source": [
        "# libraries\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# parameters for this script\n",
        "batch_size = 32\n",
        "num_classes = 27\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "\n",
        "# normalize the data\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "\n",
        "# partition training set into training and validation set\n",
        "x_validate = x_train[17301:,:]\n",
        "x_train = x_train[:17301,:]\n",
        "y_validate = y_train[17301:,:]\n",
        "y_train = y_train[:17301,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6sqrtJLzf7I"
      },
      "source": [
        "## Create CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ0QZ8cBPXu1"
      },
      "source": [
        "# Define a convolutional neural network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0KOabezkff"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB4IxRQfPXu2"
      },
      "source": [
        "def train(model, epochs, data_augmentation, opt):\n",
        "\n",
        "    # Compile the model before using it\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    # create a callback that will save the best model while training\n",
        "    save_best_model = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "    # train without data augmentation\n",
        "    if not data_augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        history = model.fit(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_validate, y_validate),\n",
        "                            shuffle=True,\n",
        "                            callbacks=[save_best_model],\n",
        "                            verbose=1)\n",
        "\n",
        "    # train with data augmentation\n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                            steps_per_epoch=math.ceil(x_train.shape[0]/batch_size),\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_validate, y_validate),\n",
        "                            callbacks=[save_best_model],\n",
        "                            verbose=0)\n",
        "    \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYXRx-9oPXu2",
        "outputId": "21e74633-1d98-4ad8-885a-a9a1ea04b436"
      },
      "source": [
        "# train and evaluate conv model with Adam\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model = create_model()\n",
        "history = train(model, epochs=40, data_augmentation=True, opt=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 80, 60, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 80, 60, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 80, 60, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 80, 60, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 40, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 40, 30, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 40, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 40, 30, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 40, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 20, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 20, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 19200)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               9830912   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 27)                13851     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 27)                0         \n",
            "=================================================================\n",
            "Total params: 9,910,331\n",
            "Trainable params: 9,910,331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Using real-time data augmentation.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.79658, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.79658 to 0.83033, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.83033\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.83033 to 0.86916, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.86916 to 0.88026, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88026 to 0.88049, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88049\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88049 to 0.89066, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89066 to 0.89459, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.89459 to 0.89852, saving model to best_model.h5\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.89852 to 0.90337, saving model to best_model.h5\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90337\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90337\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90337 to 0.90592, saving model to best_model.h5\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.90592 to 0.91147, saving model to best_model.h5\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91147\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.91147 to 0.91331, saving model to best_model.h5\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91331\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91331\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.91331 to 0.92025, saving model to best_model.h5\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.92025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ejTe5FTxO4R"
      },
      "source": [
        "# Using RNN to predict categories with noisyTextDescription"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhFEM2heuGCi"
      },
      "source": [
        "import string \n",
        "\n",
        "df_train, df_test = load_data()\n",
        "\n",
        "n_train_data = 21627\n",
        "n_test_data = 21628\n",
        "all_letters = string.ascii_letters + '0123456789&-\\' '\n",
        "n_letters = len(all_letters)\n",
        "all_categories = list(np.unique(np.array(df_train['category'])))\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "train_overall = np.array(df_train[['category', 'noisyTextDescription']])\n",
        "train_data = np.array(df_train['noisyTextDescription'])\n",
        "test_id = np.array(df_test['id'])\n",
        "test_data = np.array(df_test['noisyTextDescription'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TWLag3LuQiy"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3z0oaeEj8OG"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per category\n",
        "category_lines = {}\n",
        "train_data = {}\n",
        "validation_data = {}\n",
        "test_data = {}\n",
        "\n",
        "for category in all_categories:\n",
        "  train_data[category] = []\n",
        "\n",
        "for i in range(n_train_data):\n",
        "  category = train_overall[i][0]\n",
        "  train_data[category].append(train_overall[i][1])\n",
        "\n",
        "for category in all_categories:\n",
        "    lines = train_data[category]\n",
        "    random.shuffle(lines)\n",
        "    train_data[category] = lines[0:int(math.floor(0.7*len(lines)))]\n",
        "    validation_data[category] = lines[int(math.floor(0.7*len(lines)))+1:int(math.floor(0.85*len(lines)))]\n",
        "    test_data[category] = lines[int(math.floor(0.85*len(lines)))+1:]\n",
        "    category_lines[category] = lines\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06hmsxu2xX4Y"
      },
      "source": [
        "## Create RNN models (LSTM and GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp4OHr7VrUCu"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN_LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden, cell = self.lstm_cell(input,(hidden,cell))\n",
        "        output = self.i2o(combined)        \n",
        "        output = self.softmax(output)\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return Variable(torch.zeros(1, self.hidden_size)), Variable(torch.zeros(1, self.hidden_size))\n",
        "    \n",
        "class RNN_GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN_GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.gru_cell(input,hidden)\n",
        "        output = self.i2o(combined)        \n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return Variable(torch.zeros(1, self.hidden_size))\n",
        "    \n",
        "n_hidden = 128\n",
        "rnn_lstm = RNN_LSTM(n_letters, n_hidden, n_categories)\n",
        "rnn_gru = RNN_GRU(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3PhQDBcz6k6"
      },
      "source": [
        "## Trainning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLYj_ku7z8WS"
      },
      "source": [
        "### Prepare for trainning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KpHhLjTgo3F"
      },
      "source": [
        "criterion_lstm = nn.NLLLoss()\n",
        "criterion_gru = nn.NLLLoss()\n",
        "\n",
        "lstm_optimizer = torch.optim.Adam(rnn_lstm.parameters())\n",
        "gru_optimizer = torch.optim.Adam(rnn_gru.parameters())\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden_lstm, cell_lstm = rnn_lstm.initHidden()\n",
        "    hidden_gru = rnn_gru.initHidden()\n",
        "\n",
        "    # reset gradient\n",
        "    rnn_lstm.zero_grad()\n",
        "    rnn_gru.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output_lstm, hidden_lstm, cell_lstm = rnn_lstm(line_tensor[i], hidden_lstm, cell_lstm)\n",
        "        output_gru, hidden_gru = rnn_gru(line_tensor[i], hidden_gru)\n",
        "\n",
        "    loss_lstm = criterion_lstm(output_lstm, category_tensor)\n",
        "    loss_gru = criterion_gru(output_gru, category_tensor)\n",
        "\n",
        "    # compute gradient by backpropagation\n",
        "    loss_lstm.backward()\n",
        "    loss_gru.backward()\n",
        "\n",
        "    # update parameters\n",
        "    lstm_optimizer.step()\n",
        "    gru_optimizer.step()\n",
        "\n",
        "    return output_lstm, loss_lstm.item(), output_gru, loss_gru.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Ye9qD-hXM7",
        "outputId": "84460520-cf68-458a-8187-d6bc0dad24dc"
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "    \n",
        "import random\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(train_data[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category = Dress / line = Custom White Floral Design Dress\n",
            "category = Cufflinks / line = Jamaica Men Steel Cufflinks\n",
            "category = Topwear / line = UCB N9336SL03 NA2394SL02 Hazel Cl Top\n",
            "category = Bags / line = Nike Noisy Blue Lt.khaki France 661 Backpack\n",
            "category = Innerwear / line = Amante Women Tango Seamless Briefs PCSN02\n",
            "category = Eyewear / line = Polaroid Unisex Sunglasses\n",
            "category = Dress / line = Tonga Women Honeysuckle Dress\n",
            "category = Innerwear / line = Lovable Women Tease Pink Bra\n",
            "category = Dress / line = Avirate Women toned Dress\n",
            "category = Sandal / line = Crocs Dora Boots Pink I071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCfcu8UyueA8"
      },
      "source": [
        "def evaluate(line_tensor):\n",
        "    hidden_lstm, cell_lstm = rnn_lstm.initHidden()\n",
        "    hidden_gru = rnn_gru.initHidden()\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output_lstm, hidden_lstm, cell_lstm = rnn_lstm(line_tensor[i], hidden_lstm, cell_lstm)\n",
        "        output_gru, hidden_gru = rnn_gru(line_tensor[i], hidden_gru)\n",
        "    return output_lstm, output_gru \n",
        "\n",
        "def eval_dataset(dataset):\n",
        "    loss_lstm = 0\n",
        "    loss_gru = 0\n",
        "    n_instances = 0\n",
        "    confusion_lstm = torch.zeros(n_categories, n_categories)\n",
        "    confusion_gru = torch.zeros(n_categories, n_categories)\n",
        "    for category in all_categories:\n",
        "        category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
        "        n_instances += len(dataset[category])\n",
        "        category_i = all_categories.index(category)\n",
        "        for line in dataset[category]:\n",
        "            line_tensor = Variable(lineToTensor(line))\n",
        "            output_lstm, output_gru = evaluate(line_tensor)\n",
        "            \n",
        "            loss_lstm += criterion_lstm(output_lstm, category_tensor)\n",
        "            guess_lstm, guess_i_lstm = categoryFromOutput(output_lstm)\n",
        "            confusion_lstm[category_i][guess_i_lstm] += 1\n",
        "\n",
        "            loss_gru += criterion_gru(output_gru, category_tensor)\n",
        "            guess_gru, guess_i_gru = categoryFromOutput(output_gru)\n",
        "            confusion_gru[category_i][guess_i_gru] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(n_categories):\n",
        "        confusion_lstm[i] = confusion_lstm[i] / confusion_lstm[i].sum()\n",
        "        confusion_gru[i] = confusion_gru[i] / confusion_gru[i].sum()\n",
        "\n",
        "    return loss_lstm.item() / n_instances, confusion_lstm, loss_gru.item() / n_instances, confusion_gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeg8GIIjhAH1",
        "outputId": "129e9c49-bdc5-4bbd-b865-d9fb1b3b1fbc"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 20000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "train_loss_lstm = 0\n",
        "train_loss_gru = 0\n",
        "all_train_losses_lstm = []\n",
        "all_train_losses_gru = []\n",
        "all_validation_losses_lstm = []\n",
        "all_validation_losses_gru = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "  \n",
        "print('\\nIter \\tTrain% \\tTime \\t \\tTrain_loss_LSTM \\tTrain_loss_GRU \\tExample')\n",
        "start = time.time()\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output_lstm, loss_lstm, output_gru, loss_gru = train(category_tensor, line_tensor)\n",
        "    train_loss_lstm += loss_lstm\n",
        "    train_loss_gru += loss_gru\n",
        "\n",
        "    # Print iter number, train loss average, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output_lstm)\n",
        "        correct = 'âœ“' if guess == category else 'âœ— (%s)' % category\n",
        "        print('%d \\t%d%% \\t(%s) \\t%.4f \\t\\t\\t%.4f \\t\\t%s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), train_loss_lstm / plot_every, train_loss_gru / plot_every, line, guess, correct))\n",
        "\n",
        "    # Add current train loss average to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_train_losses_lstm.append(train_loss_lstm / plot_every)\n",
        "        all_train_losses_gru.append(train_loss_gru / plot_every)\n",
        "        train_loss_lstm = 0\n",
        "        train_loss_gru = 0\n",
        "        \n",
        "    # Compute loss based on validation data\n",
        "    if iter % plot_every == 0:\n",
        "        average_validation_loss_lstm, _, average_validation_loss_gru, _ = eval_dataset(validation_data)\n",
        "\n",
        "        # save model with best validation loss\n",
        "        if len(all_validation_losses_lstm) == 0 or average_validation_loss_lstm < min(all_validation_losses_lstm):\n",
        "            torch.save(rnn_lstm, 'char_rnn_classification_model_lstm.pt')\n",
        "        all_validation_losses_lstm.append(average_validation_loss_lstm)\n",
        "        if len(all_validation_losses_gru) == 0 or average_validation_loss_gru < min(all_validation_losses_gru):\n",
        "            torch.save(rnn_gru, 'char_rnn_classification_model_gru.pt')\n",
        "        all_validation_losses_gru.append(average_validation_loss_gru)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iter \tTrain% \tTime \t \tTrain_loss_LSTM \tTrain_loss_GRU \tExample\n",
            "5000 \t25% \t(5m 47s) \t1.1422 \t\t\t1.0697 \t\tWater Chronograph Sandwichtost MW-600F-7AVDF(A507) Watch / Watches âœ— (Free Gifts)\n",
            "10000 \t50% \t(12m 34s) \t0.9300 \t\t\t0.7907 \t\tFossil Men Brown Wallet / Wallets âœ“\n",
            "15000 \t75% \t(19m 26s) \t0.7321 \t\t\t0.6133 \t\tMurcia Women Brown & White Bag / Accessories âœ— (Bags)\n",
            "20000 \t100% \t(26m 20s) \t0.6608 \t\t\t0.5355 \t\tLotus ace Plum Mist Nail Polish 952 / Nails âœ“\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxx74U0vup9m"
      },
      "source": [
        "gru_data = []\n",
        "lstm_data = []\n",
        "for i in range(n_test_data):\n",
        "  line = test_data[i]\n",
        "  line_tensor = lineToTensor(line)\n",
        "  output_lstm, output_gru = evaluate(line_tensor)\n",
        "  lstm_data.append(output_lstm)\n",
        "  gru_data.append(output_gru)\n",
        "  \n",
        "df = pd.DataFrame(result_data, columns=['id', 'category'])\n",
        "df.to_csv('submission_lstm.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQnuJ2mo6xdW"
      },
      "source": [
        "def bin_to_category(predict_data, lstm_data):\n",
        "  n_data = len(predict_data)\n",
        "  y_test = []\n",
        "  for i in range(n_data):\n",
        "    index = np.argmax(predict_data[i])\n",
        "    largest_value = predict_data[i][index]\n",
        "    predict_data[i][index] = 0\n",
        "    index2 = np.argmax(predict_data[i])\n",
        "    largest_value_2 = predict_data[i][index2]\n",
        "    if largest_value * 0.8 < largest_value_2:\n",
        "      if lstm_data[i] == index or lstm_data[i] == index2:\n",
        "        y_test.append(all_categories[lstm_data[i]])\n",
        "      else:\n",
        "        y_test.append(all_categories[index])\n",
        "    else:\n",
        "      y_test.append(all_categories[index])\n",
        "  return y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeDIHV0f9z4Y"
      },
      "source": [
        "def bin_to_category2(predict_data):\n",
        "  n_data = len(predict_data)\n",
        "  y_test = []\n",
        "  for i in range(n_data):\n",
        "    index = np.argmax(predict_data[i])\n",
        "    y_test.append(all_categories[index])\n",
        "  return y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZamIg993HGH",
        "outputId": "670e2255-685f-4042-859a-2fd98f626d64"
      },
      "source": [
        "saved_model = load_model('best_model.h5')\n",
        "predict_val = saved_model.predict(x_test)\n",
        "y_test = bin_to_category2(predict_val)\n",
        "cnn_data = []\n",
        "\n",
        "print(y_test[: 15])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apparel Set', 'Apparel Set', 'Scarves', 'Flip Flops', 'Free Gifts', 'Flip Flops', 'Apparel Set', 'Saree', 'Ties', 'Flip Flops', 'Free Gifts', 'Apparel Set', 'Bags', 'Free Gifts', 'Dress']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltDBe6a09JrF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}