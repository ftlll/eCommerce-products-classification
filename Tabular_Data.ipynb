{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alVgMngbxg6n"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "-4TZlidR-qR0",
        "outputId": "a5e636f9-ff76-431d-b865-84a88969a5fb"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bdcec111-b750-4347-ba29-a856bc16d7d4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bdcec111-b750-4347-ba29-a856bc16d7d4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhIMMi829_k8"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKbuEv3TxeGd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import feature_column\n",
        "\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim import Adam, SGD\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nr4ZKj9L5Y"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drxO8iad5UKO"
      },
      "source": [
        "def load_data():\n",
        "  train_file = \"train.csv\"\n",
        "  test_file = \"test.csv\"\n",
        "  df_train = pd.read_csv(train_file)\n",
        "  df_test = pd.read_csv(test_file)\n",
        "\n",
        "  return df_train, df_test"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FRafdip5b0a"
      },
      "source": [
        "df_train, df_test = load_data()\n",
        "\n",
        "# We do not need description for this part\n",
        "train_data = df_train.drop(['noisyTextDescription'],axis=1)\n",
        "test_data = df_test.drop(['id','noisyTextDescription'],axis=1)\n",
        "y_train = df_train['category']"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTHaDRauQEIs"
      },
      "source": [
        "categorical_columns = ['gender', 'baseColour', 'season', 'usage']\n",
        "y_columns = ['category']\n",
        "\n",
        "train_categorys = df_train[\"category\"]\n",
        "all_categorys = list(set(train_categorys))\n",
        "n_categorys = len(all_categorys) # 27\n",
        "\n",
        "train_genders = df_train[\"gender\"]\n",
        "all_genders = list(set(train_genders))\n",
        "n_genders = len(all_genders) # 5\n",
        "\n",
        "train_baseColour = df_train[\"baseColour\"]\n",
        "all_baseColour = list(set(train_baseColour))\n",
        "n_baseColour = len(all_baseColour) # 46\n",
        "\n",
        "train_seasons = df_train[\"season\"]\n",
        "all_seasons= list(set(train_seasons))\n",
        "n_seasons = len(all_seasons) # 4\n",
        "\n",
        "train_usages = df_train[\"usage\"]\n",
        "all_usages = list(set(train_usages))\n",
        "n_usages = len(all_usages) # 7\n",
        "\n",
        "train_usages = df_train[\"usage\"]\n",
        "all_usages = list(set(train_usages))\n",
        "n_usages = len(all_usages) # 7"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGZfDnL_kyN"
      },
      "source": [
        "### Use label encoder to preprocess tabular data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlNA_ux89TjG"
      },
      "source": [
        "GENDER_ENCODER = LabelEncoder()\n",
        "GENDER_ENCODER.fit(train_data[\"gender\"])\n",
        "train_data['gender'] = GENDER_ENCODER.transform(train_data['gender'])\n",
        "test_data['gender'] = GENDER_ENCODER.transform(test_data['gender'])\n",
        "\n",
        "SEASON_ENCODER = LabelEncoder()\n",
        "SEASON_ENCODER.fit(train_data[\"season\"])\n",
        "train_data['season'] = SEASON_ENCODER.transform(train_data['season'])\n",
        "test_data['season'] = SEASON_ENCODER.transform(test_data['season'])\n",
        "\n",
        "COLOR_ENCODER = LabelEncoder()\n",
        "COLOR_ENCODER.fit(train_data[\"baseColour\"])\n",
        "train_data['baseColour'] = COLOR_ENCODER.transform(train_data['baseColour'])\n",
        "test_data['baseColour'] = COLOR_ENCODER.transform(test_data['baseColour'])\n",
        "\n",
        "USAGE_ENCODER = LabelEncoder()\n",
        "USAGE_ENCODER.fit(train_data[\"usage\"])\n",
        "train_data['usage'] = USAGE_ENCODER.transform(train_data['usage'])\n",
        "test_data['usage'] = USAGE_ENCODER.transform(test_data['usage'])\n",
        "\n",
        "CATEGORY_ENCODER = LabelEncoder()\n",
        "CATEGORY_ENCODER.fit(y_train)\n",
        "train_data['category'] = CATEGORY_ENCODER.transform(train_data['category'])\n",
        "y_train = CATEGORY_ENCODER.transform(y_train)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuHpNov4x-pP"
      },
      "source": [
        "train_df, valid_df = train_test_split(train_data, test_size=0.2, random_state=11)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeTS5SKFyDvz"
      },
      "source": [
        "train_df.reset_index(drop=True, inplace=True)\n",
        "valid_df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHH6yOizyIbH"
      },
      "source": [
        "class Table_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.IDs = list(df['id'])\n",
        "        self.genders = list(df['gender'])\n",
        "        self.seasons = list(df['season'])\n",
        "        self.colors = list(df['baseColour'])\n",
        "        self.usages = list(df['usage'])\n",
        "        self.labels = list(df['category'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.IDs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # read X\n",
        "        label = self.labels[idx]\n",
        "        row_data = []\n",
        "        features = torch.Tensor([self.genders[idx], self.seasons[idx], self.colors[idx], self.usages[idx]])\n",
        "\n",
        "        return features, label"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UaoB1c_Szch"
      },
      "source": [
        "train_dataset = Table_Dataset(train_df)\n",
        "train_iterator = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "valid_dataset = Table_Dataset(valid_df)\n",
        "valid_iterator = DataLoader(valid_dataset, batch_size=512, shuffle=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yHCOwYZ9PF2"
      },
      "source": [
        "# Create NN model and related methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMb0I_xstx_n"
      },
      "source": [
        "class NN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size) :\n",
        "        super().__init__()\n",
        "        hidden_size = 27\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc7 = nn.Linear(hidden_size+4, hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        temp = x\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc6(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = torch.cat((temp, x), dim=1)\n",
        "\n",
        "        x = self.fc7(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x.squeeze()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8CgkkzL5tDK"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for feature, label in iterator:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        y_pred = model(feature)\n",
        "        \n",
        "        \n",
        "        loss = criterion(y_pred, label)\n",
        "        \n",
        "        acc = calculate_accuracy(y_pred, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate_nn(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for feature, label in iterator:\n",
        "\n",
        "            y_pred = model(feature)\n",
        "            \n",
        "            loss = criterion(y_pred, label)\n",
        "            \n",
        "            acc = calculate_accuracy(y_pred, label)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuwhiuC67cGh"
      },
      "source": [
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "model = NN(input_size=4, hidden_size=15, output_size=27)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvu8lH8z7jdG",
        "outputId": "34aff3fb-1bfa-4d92-c830-149a3ae4bb24"
      },
      "source": [
        "train_accuracy_list = []\n",
        "train_loss_list = []\n",
        "valid_acc_list = []\n",
        "valid_loss_list = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    # train\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # valid\n",
        "    valid_loss, valid_acc = evaluate_nn(model, valid_iterator, criterion)\n",
        "\n",
        "    # save best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model, 'linear-model.pt')\n",
        "\n",
        "    # Track the accuracy\n",
        "    train_accuracy_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "    valid_loss_list.append(valid_loss)\n",
        "        \n",
        "    # print epoch info\n",
        "    end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 3.248 | Train Acc: 17.03%\n",
            "\t Val. Loss: 2.647 |  Val. Acc: 33.19%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.516 | Train Acc: 33.07%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 32.69%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.352 | Train Acc: 34.88%\n",
            "\t Val. Loss: 2.297 |  Val. Acc: 36.15%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.297 | Train Acc: 35.85%\n",
            "\t Val. Loss: 2.233 |  Val. Acc: 37.35%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.229 | Train Acc: 36.68%\n",
            "\t Val. Loss: 2.165 |  Val. Acc: 40.09%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.168 | Train Acc: 38.44%\n",
            "\t Val. Loss: 2.109 |  Val. Acc: 40.24%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.122 | Train Acc: 40.02%\n",
            "\t Val. Loss: 2.074 |  Val. Acc: 42.38%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.080 | Train Acc: 42.18%\n",
            "\t Val. Loss: 2.029 |  Val. Acc: 43.17%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.039 | Train Acc: 42.88%\n",
            "\t Val. Loss: 1.992 |  Val. Acc: 43.86%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.005 | Train Acc: 43.65%\n",
            "\t Val. Loss: 1.957 |  Val. Acc: 44.55%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.975 | Train Acc: 44.41%\n",
            "\t Val. Loss: 1.943 |  Val. Acc: 44.71%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.952 | Train Acc: 44.56%\n",
            "\t Val. Loss: 1.919 |  Val. Acc: 45.17%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.936 | Train Acc: 44.69%\n",
            "\t Val. Loss: 1.900 |  Val. Acc: 44.89%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.922 | Train Acc: 44.55%\n",
            "\t Val. Loss: 1.901 |  Val. Acc: 44.67%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.912 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.891 |  Val. Acc: 44.83%\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.902 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.886 |  Val. Acc: 45.23%\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.891 | Train Acc: 44.54%\n",
            "\t Val. Loss: 1.856 |  Val. Acc: 45.52%\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.885 | Train Acc: 44.74%\n",
            "\t Val. Loss: 1.869 |  Val. Acc: 44.58%\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.881 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.846 |  Val. Acc: 45.60%\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.862 | Train Acc: 45.00%\n",
            "\t Val. Loss: 1.838 |  Val. Acc: 44.90%\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.852 | Train Acc: 44.80%\n",
            "\t Val. Loss: 1.832 |  Val. Acc: 45.15%\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.846 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.838 |  Val. Acc: 44.02%\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.838 | Train Acc: 45.42%\n",
            "\t Val. Loss: 1.805 |  Val. Acc: 45.73%\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.825 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.799 |  Val. Acc: 45.15%\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.815 | Train Acc: 45.72%\n",
            "\t Val. Loss: 1.793 |  Val. Acc: 45.84%\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.805 | Train Acc: 45.53%\n",
            "\t Val. Loss: 1.795 |  Val. Acc: 45.49%\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.799 | Train Acc: 45.51%\n",
            "\t Val. Loss: 1.789 |  Val. Acc: 44.75%\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.788 | Train Acc: 45.92%\n",
            "\t Val. Loss: 1.767 |  Val. Acc: 46.34%\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.783 | Train Acc: 46.28%\n",
            "\t Val. Loss: 1.769 |  Val. Acc: 46.30%\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.779 | Train Acc: 46.44%\n",
            "\t Val. Loss: 1.759 |  Val. Acc: 46.00%\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.768 | Train Acc: 47.22%\n",
            "\t Val. Loss: 1.755 |  Val. Acc: 46.65%\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.763 | Train Acc: 47.33%\n",
            "\t Val. Loss: 1.743 |  Val. Acc: 46.72%\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.751 | Train Acc: 47.63%\n",
            "\t Val. Loss: 1.742 |  Val. Acc: 46.53%\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.749 | Train Acc: 47.64%\n",
            "\t Val. Loss: 1.738 |  Val. Acc: 48.13%\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.750 | Train Acc: 47.70%\n",
            "\t Val. Loss: 1.748 |  Val. Acc: 47.47%\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.744 | Train Acc: 47.64%\n",
            "\t Val. Loss: 1.729 |  Val. Acc: 47.99%\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.735 | Train Acc: 48.01%\n",
            "\t Val. Loss: 1.720 |  Val. Acc: 47.69%\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.732 | Train Acc: 47.94%\n",
            "\t Val. Loss: 1.715 |  Val. Acc: 47.90%\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.727 | Train Acc: 48.25%\n",
            "\t Val. Loss: 1.714 |  Val. Acc: 48.42%\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.729 | Train Acc: 47.92%\n",
            "\t Val. Loss: 1.717 |  Val. Acc: 47.77%\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.719 | Train Acc: 48.62%\n",
            "\t Val. Loss: 1.703 |  Val. Acc: 47.93%\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.720 | Train Acc: 48.18%\n",
            "\t Val. Loss: 1.713 |  Val. Acc: 47.77%\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.718 | Train Acc: 48.24%\n",
            "\t Val. Loss: 1.712 |  Val. Acc: 46.86%\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.714 | Train Acc: 48.30%\n",
            "\t Val. Loss: 1.702 |  Val. Acc: 48.67%\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.708 | Train Acc: 48.41%\n",
            "\t Val. Loss: 1.700 |  Val. Acc: 47.47%\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.710 | Train Acc: 48.34%\n",
            "\t Val. Loss: 1.693 |  Val. Acc: 48.04%\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.702 | Train Acc: 48.68%\n",
            "\t Val. Loss: 1.703 |  Val. Acc: 48.15%\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.705 | Train Acc: 48.33%\n",
            "\t Val. Loss: 1.705 |  Val. Acc: 47.87%\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.706 | Train Acc: 48.19%\n",
            "\t Val. Loss: 1.693 |  Val. Acc: 48.68%\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.702 | Train Acc: 48.39%\n",
            "\t Val. Loss: 1.691 |  Val. Acc: 48.66%\n",
            "Epoch: 51 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.700 | Train Acc: 48.41%\n",
            "\t Val. Loss: 1.696 |  Val. Acc: 48.10%\n",
            "Epoch: 52 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.693 | Train Acc: 48.75%\n",
            "\t Val. Loss: 1.682 |  Val. Acc: 48.01%\n",
            "Epoch: 53 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.690 | Train Acc: 48.78%\n",
            "\t Val. Loss: 1.691 |  Val. Acc: 47.65%\n",
            "Epoch: 54 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.695 | Train Acc: 48.43%\n",
            "\t Val. Loss: 1.688 |  Val. Acc: 48.02%\n",
            "Epoch: 55 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.688 | Train Acc: 48.77%\n",
            "\t Val. Loss: 1.688 |  Val. Acc: 48.40%\n",
            "Epoch: 56 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.691 | Train Acc: 48.44%\n",
            "\t Val. Loss: 1.707 |  Val. Acc: 46.25%\n",
            "Epoch: 57 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.688 | Train Acc: 48.65%\n",
            "\t Val. Loss: 1.682 |  Val. Acc: 48.71%\n",
            "Epoch: 58 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.680 | Train Acc: 49.08%\n",
            "\t Val. Loss: 1.674 |  Val. Acc: 48.93%\n",
            "Epoch: 59 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.688 | Train Acc: 48.66%\n",
            "\t Val. Loss: 1.678 |  Val. Acc: 47.74%\n",
            "Epoch: 60 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.675 | Train Acc: 48.89%\n",
            "\t Val. Loss: 1.666 |  Val. Acc: 48.45%\n",
            "Epoch: 61 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.675 | Train Acc: 48.86%\n",
            "\t Val. Loss: 1.666 |  Val. Acc: 48.94%\n",
            "Epoch: 62 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.675 | Train Acc: 49.07%\n",
            "\t Val. Loss: 1.666 |  Val. Acc: 48.58%\n",
            "Epoch: 63 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.669 | Train Acc: 49.19%\n",
            "\t Val. Loss: 1.681 |  Val. Acc: 47.76%\n",
            "Epoch: 64 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.672 | Train Acc: 49.12%\n",
            "\t Val. Loss: 1.658 |  Val. Acc: 48.72%\n",
            "Epoch: 65 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.665 | Train Acc: 49.40%\n",
            "\t Val. Loss: 1.669 |  Val. Acc: 48.92%\n",
            "Epoch: 66 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.665 | Train Acc: 49.64%\n",
            "\t Val. Loss: 1.654 |  Val. Acc: 49.33%\n",
            "Epoch: 67 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.663 | Train Acc: 49.24%\n",
            "\t Val. Loss: 1.660 |  Val. Acc: 49.59%\n",
            "Epoch: 68 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.660 | Train Acc: 49.67%\n",
            "\t Val. Loss: 1.660 |  Val. Acc: 49.31%\n",
            "Epoch: 69 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.657 | Train Acc: 49.54%\n",
            "\t Val. Loss: 1.649 |  Val. Acc: 49.85%\n",
            "Epoch: 70 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.655 | Train Acc: 49.76%\n",
            "\t Val. Loss: 1.663 |  Val. Acc: 49.40%\n",
            "Epoch: 71 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.656 | Train Acc: 49.74%\n",
            "\t Val. Loss: 1.647 |  Val. Acc: 49.67%\n",
            "Epoch: 72 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.651 | Train Acc: 49.77%\n",
            "\t Val. Loss: 1.658 |  Val. Acc: 49.02%\n",
            "Epoch: 73 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.648 | Train Acc: 50.02%\n",
            "\t Val. Loss: 1.651 |  Val. Acc: 49.47%\n",
            "Epoch: 74 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.649 | Train Acc: 49.88%\n",
            "\t Val. Loss: 1.663 |  Val. Acc: 49.38%\n",
            "Epoch: 75 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.646 | Train Acc: 49.74%\n",
            "\t Val. Loss: 1.651 |  Val. Acc: 49.77%\n",
            "Epoch: 76 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.650 | Train Acc: 49.93%\n",
            "\t Val. Loss: 1.647 |  Val. Acc: 49.45%\n",
            "Epoch: 77 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.644 | Train Acc: 49.94%\n",
            "\t Val. Loss: 1.646 |  Val. Acc: 49.24%\n",
            "Epoch: 78 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.644 | Train Acc: 49.88%\n",
            "\t Val. Loss: 1.642 |  Val. Acc: 50.07%\n",
            "Epoch: 79 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.637 | Train Acc: 49.90%\n",
            "\t Val. Loss: 1.637 |  Val. Acc: 49.25%\n",
            "Epoch: 80 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.635 | Train Acc: 49.96%\n",
            "\t Val. Loss: 1.645 |  Val. Acc: 49.77%\n",
            "Epoch: 81 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.635 | Train Acc: 49.92%\n",
            "\t Val. Loss: 1.639 |  Val. Acc: 49.71%\n",
            "Epoch: 82 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.633 | Train Acc: 50.07%\n",
            "\t Val. Loss: 1.644 |  Val. Acc: 49.37%\n",
            "Epoch: 83 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.628 | Train Acc: 50.06%\n",
            "\t Val. Loss: 1.630 |  Val. Acc: 49.52%\n",
            "Epoch: 84 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.630 | Train Acc: 50.17%\n",
            "\t Val. Loss: 1.634 |  Val. Acc: 49.64%\n",
            "Epoch: 85 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.634 | Train Acc: 49.93%\n",
            "\t Val. Loss: 1.640 |  Val. Acc: 49.54%\n",
            "Epoch: 86 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.627 | Train Acc: 50.13%\n",
            "\t Val. Loss: 1.634 |  Val. Acc: 49.62%\n",
            "Epoch: 87 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.625 | Train Acc: 50.31%\n",
            "\t Val. Loss: 1.624 |  Val. Acc: 50.01%\n",
            "Epoch: 88 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.629 | Train Acc: 50.18%\n",
            "\t Val. Loss: 1.618 |  Val. Acc: 50.19%\n",
            "Epoch: 89 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.622 | Train Acc: 50.13%\n",
            "\t Val. Loss: 1.633 |  Val. Acc: 49.90%\n",
            "Epoch: 90 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.624 | Train Acc: 50.16%\n",
            "\t Val. Loss: 1.627 |  Val. Acc: 49.50%\n",
            "Epoch: 91 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.619 | Train Acc: 50.37%\n",
            "\t Val. Loss: 1.621 |  Val. Acc: 50.16%\n",
            "Epoch: 92 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.621 | Train Acc: 50.28%\n",
            "\t Val. Loss: 1.620 |  Val. Acc: 49.84%\n",
            "Epoch: 93 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.615 | Train Acc: 50.37%\n",
            "\t Val. Loss: 1.622 |  Val. Acc: 49.98%\n",
            "Epoch: 94 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.615 | Train Acc: 50.33%\n",
            "\t Val. Loss: 1.631 |  Val. Acc: 49.70%\n",
            "Epoch: 95 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.613 | Train Acc: 50.24%\n",
            "\t Val. Loss: 1.628 |  Val. Acc: 50.12%\n",
            "Epoch: 96 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.614 | Train Acc: 50.62%\n",
            "\t Val. Loss: 1.618 |  Val. Acc: 50.05%\n",
            "Epoch: 97 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.612 | Train Acc: 50.50%\n",
            "\t Val. Loss: 1.609 |  Val. Acc: 50.96%\n",
            "Epoch: 98 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.609 | Train Acc: 50.64%\n",
            "\t Val. Loss: 1.616 |  Val. Acc: 50.34%\n",
            "Epoch: 99 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.611 | Train Acc: 50.54%\n",
            "\t Val. Loss: 1.623 |  Val. Acc: 50.14%\n",
            "Epoch: 100 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.608 | Train Acc: 50.42%\n",
            "\t Val. Loss: 1.616 |  Val. Acc: 50.25%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msrY8Eqd7kDp"
      },
      "source": [
        "# One hot encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Ul5mUVTHx7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, label_train, label_valid = train_test_split(data_onehot, y_labels, test_size=0.3, random_state=0, stratify=y_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbusyEizZFss"
      },
      "source": [
        "def bin_to_category(predict_data):\n",
        "  n_data = len(predict_data)\n",
        "  y_test = []\n",
        "  for i in range(n_data):\n",
        "    index = np.argmax(predict_data[i])\n",
        "    y_test.append(all_categorys[index])\n",
        "  return y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhWlcC74U_qv"
      },
      "source": [
        "LGBM_data = []\n",
        "test_id = df_test['id']\n",
        "y_res = bin_to_category(y_pred)\n",
        "\n",
        "for i in range(21628):\n",
        "  row_data = []\n",
        "  row_data.append(int(test_id[i]))\n",
        "  row_data.append(y_res[i])\n",
        "  LGBM_data.append(row_data)\n",
        "\n",
        "df = pd.DataFrame(LGBM_data, columns=['id', 'category'])\n",
        "df.to_csv('LGBM.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}